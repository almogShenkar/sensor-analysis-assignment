{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f6ad58",
   "metadata": {},
   "source": [
    "\n",
    "# IMU Classification (Base) — **Simple & Robust Mapping**\n",
    "\n",
    "**Assumption:** CSV labels are only `normal` and `collision`.**Goal:** Keep mapping trivial, avoid `classes_` entirely.\n",
    "\n",
    "We infer which column of `predict_proba` corresponds to **`collision`** by choosing the column\n",
    "that yields a **higher ROC-AUC** against `y_test_bin = (y_test == \"collision\")`.\n",
    "This removes any dependence on internal class encoding and avoids brittle assertions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "651392b9-a6d7-44c6-bdeb-68fbe5a5a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_features import process_dataset, load_and_process_sample\n",
    "from visualization import signal_viewer\n",
    "from imu_pipeline import IMUPipeline\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9efae6be-e1e7-49ca-95ce-a98d8c116901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c522b51f0154bd0829fd3f68220fe56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Sample ID:', layout=Layout(width='50%'), options=('00104b76-d512-43d6-b2e…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "signal_viewer(\n",
    "    data_dir=Path('data/raw/train'),\n",
    "    labels_csv=Path('data/train.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5197a4-4c56-4f84-bf7b-841ca6fd9a08",
   "metadata": {},
   "source": [
    "# ❓ Questions to Reflect On\n",
    "What do you observe when comparing the model’s predictions on the new data to its known performance?\n",
    "\n",
    "Is there anything in the data that might explain differences in behavior?\n",
    "\n",
    "Can you identify patterns or trends related to when the model succeeds or fails?\n",
    "\n",
    "Are there signals or features that seem to affect the model’s reliability?\n",
    "\n",
    "What could be done in the short term to handle the current situation?\n",
    "\n",
    "What are potential long-term steps to improve model performance in similar scenarios?\n",
    "\n",
    "What would you want to explore further if given more time or data?\n",
    "\n",
    "What assumptions did the model rely on during training — and are they still valid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b76641",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Setup\n",
    "import os, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix\n",
    ")\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "MANUAL_ANN = DATA_DIR / \"manual_annotation\"\n",
    "MODEL_PATH = Path(\"models/imu_pipeline.pkl\")\n",
    "\n",
    "TRAIN_CSV = DATA_DIR / \"train.csv\"\n",
    "TEST_CSV  = DATA_DIR / \"test.csv\"\n",
    "INF_CSV   = DATA_DIR / \"inference.csv\"\n",
    "INF_LABELS_CSV = MANUAL_ANN / \"inference_labels.csv\"\n",
    "\n",
    "assert TRAIN_CSV.exists(), f\"Missing {TRAIN_CSV}\"\n",
    "assert TEST_CSV.exists(),  f\"Missing {TEST_CSV}\"\n",
    "assert MODEL_PATH.exists(), f\"Missing {MODEL_PATH}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27cf31aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- (Optional) feature generation for inference\n",
    "if not INF_CSV.exists():\n",
    "    try:\n",
    "        from extract_features import process_dataset\n",
    "        print(\"Generating inference features ...\")\n",
    "        process_dataset(\"inference\")\n",
    "        print(\"Done. Created:\", INF_CSV)\n",
    "    except Exception as e:\n",
    "        print(\"Couldn't generate inference features automatically. Reason:\", e)\n",
    "        print(\"Please run: from extract_features import process_dataset; process_dataset('inference')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9e7a425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     x_mean     x_std     x_max     x_min   x_range    x_skew  x_kurtosis  \\\n",
       " 0  0.057289  0.140447  0.265758 -0.214983  0.480742 -0.388161   -1.292653   \n",
       " 1  0.058397  0.147305  0.315987 -0.237875  0.553862 -0.371543   -1.247677   \n",
       " \n",
       "    x_n_peaks  x_energy    y_mean  ...  temperature  humidity  altitude  \\\n",
       " 0         17  2.300756  0.014241  ...           20        46       409   \n",
       " 1         12  2.510906  0.015394  ...           18        34       632   \n",
       " \n",
       "    session_id  firmware_version  calibration_status  battery_level  \\\n",
       " 0     S607704            v1.2.3              recent             93   \n",
       " 1     S513749            v1.3.0              recent             94   \n",
       " \n",
       "    gps_accuracy  network_type  device_model  \n",
       " 0      6.111424            5g       model_c  \n",
       " 1      5.448867          wifi       model_c  \n",
       " \n",
       " [2 rows x 61 columns],\n",
       "      x_mean     x_std     x_max     x_min   x_range    x_skew  x_kurtosis  \\\n",
       " 0  0.055139  0.149892  0.277077 -0.204022  0.481099 -0.298338   -1.362385   \n",
       " 1  0.061628  0.143698  0.278498 -0.209266  0.487764 -0.387841   -1.318381   \n",
       " \n",
       "    x_n_peaks  x_energy    y_mean  ...  temperature  humidity  altitude  \\\n",
       " 0         14  2.550784  0.017328  ...           33        29       814   \n",
       " 1         18  2.444723  0.017545  ...           33        59       264   \n",
       " \n",
       "    session_id  firmware_version  calibration_status  battery_level  \\\n",
       " 0     S186189            v1.3.0              recent             94   \n",
       " 1     S799382            v1.2.3                  ok             22   \n",
       " \n",
       "    gps_accuracy  network_type  device_model  \n",
       " 0      9.497950          wifi       model_b  \n",
       " 1      6.290437            5g       model_b  \n",
       " \n",
       " [2 rows x 61 columns])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Load data\n",
    "train = pd.read_csv(TRAIN_CSV)\n",
    "test  = pd.read_csv(TEST_CSV)\n",
    "\n",
    "inf_labels = pd.read_csv(INF_LABELS_CSV) if INF_LABELS_CSV.exists() else None\n",
    "inf = pd.read_csv(INF_CSV) if INF_CSV.exists() else None\n",
    "\n",
    "train.head(2), test.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fb0e09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 60) (800,)\n",
      "(200, 60) (200,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Utility: split features/labels\n",
    "def split_xy(df, label_col=\"label\"):\n",
    "    X = df.drop(columns=[c for c in df.columns if c == label_col], errors=\"ignore\")\n",
    "    y = df[label_col] if label_col in df.columns else None\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = split_xy(train, \"label\")\n",
    "X_test,  y_test  = split_xy(test,  \"label\")\n",
    "print(X_train.shape, y_train.shape if y_train is not None else None)\n",
    "print(X_test.shape,  y_test.shape if y_test is not None else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b3ef449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC(proba[:,0])=0.928, AUC(proba[:,1])=0.072  -> using column 0 as P(collision)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Load model and pick which column is 'collision' by maximizing ROC-AUC on test\n",
    "model = joblib.load(MODEL_PATH)\n",
    "\n",
    "def pick_collision_index(model, X, y):\n",
    "    proba = model.predict_proba(X)\n",
    "    if proba.ndim != 2 or proba.shape[1] != 2:\n",
    "        raise ValueError(f\"Expected binary predict_proba with 2 columns, got shape={proba.shape}\")\n",
    "    y_bin = (y == \"collision\").astype(int)\n",
    "    aucs = [roc_auc_score(y_bin, proba[:, i]) for i in range(2)]\n",
    "    pos_idx = int(np.argmax(aucs))\n",
    "    print(f\"AUC(proba[:,0])={aucs[0]:.3f}, AUC(proba[:,1])={aucs[1]:.3f}  -> using column {pos_idx} as P(collision)\")\n",
    "    return pos_idx\n",
    "\n",
    "POS_IDX = pick_collision_index(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7f93ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST metrics (threshold=0.5) ===\n",
      "Accuracy=0.945  Precision=1.000  Recall=0.890  F1=0.942\n",
      "ROC-AUC=0.928   PR-AUC=0.956\n",
      "Confusion Matrix (rows=true, cols=pred) [0,1]:\n",
      " [[100   0]\n",
      " [ 11  89]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Evaluate on TEST set (simple & consistent)\n",
    "y_test_bin = (y_test == \"collision\").astype(int)      # {0,1}\n",
    "\n",
    "y_prob_test = model.predict_proba(X_test)[:, POS_IDX] # P(collision)\n",
    "y_hat_test  = (y_prob_test >= 0.5).astype(int)        # {0,1}\n",
    "\n",
    "acc = accuracy_score(y_test_bin, y_hat_test)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_test_bin, y_hat_test, average=\"binary\", zero_division=0)\n",
    "roc = roc_auc_score(y_test_bin, y_prob_test)\n",
    "pr  = average_precision_score(y_test_bin, y_prob_test)\n",
    "cm  = confusion_matrix(y_test_bin, y_hat_test, labels=[0,1])\n",
    "\n",
    "print(\"=== TEST metrics (threshold=0.5) ===\")\n",
    "print(f\"Accuracy={acc:.3f}  Precision={prec:.3f}  Recall={rec:.3f}  F1={f1:.3f}\")\n",
    "print(f\"ROC-AUC={roc:.3f}   PR-AUC={pr:.3f}\")\n",
    "print(\"Confusion Matrix (rows=true, cols=pred) [0,1]:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f915cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INFERENCE metrics (threshold=0.5) ===\n",
      "{'accuracy': 0.6448202959830867, 'precision': 0.5885416666666666, 'recall': 0.9576271186440678, 'f1': 0.7290322580645161, 'roc_auc': 0.7610670099406421, 'pr_auc': 0.7615169345222946}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Evaluate on INFERENCE set (if labels are available)\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "\n",
    "if inf is None:\n",
    "    print(\"No inference.csv; skipping inference evaluation.\")\n",
    "else:\n",
    "    # Merge labels if present\n",
    "    if inf_labels is not None:\n",
    "        key_candidates = [c for c in [\"id\",\"uuid\",\"file\",\"filename\",\"clip_id\",\"sample_id\"]\n",
    "                          if c in inf.columns and c in inf_labels.columns]\n",
    "        if key_candidates:\n",
    "            key = key_candidates[0]\n",
    "            merged = inf.merge(inf_labels, on=key, how=\"inner\", suffixes=(\"\", \"_y\"))\n",
    "            label_col = \"label\" if \"label\" in merged.columns else (\"y\" if \"y\" in merged.columns else None)\n",
    "            X_inf, y_inf = split_xy(merged, label_col=label_col)\n",
    "        else:\n",
    "            label_col = \"label\" if \"label\" in inf.columns else None\n",
    "            X_inf, y_inf = split_xy(inf, label_col=label_col)\n",
    "    else:\n",
    "        label_col = \"label\" if \"label\" in inf.columns else None\n",
    "        X_inf, y_inf = split_xy(inf, label_col=label_col)\n",
    "\n",
    "    y_prob_inf = model.predict_proba(X_inf)[:, POS_IDX]   # P(collision)\n",
    "    y_hat_inf  = (y_prob_inf >= 0.5).astype(int)\n",
    "\n",
    "    if y_inf is not None:\n",
    "        y_inf_bin = (y_inf == \"collision\").astype(int)\n",
    "        metrics_inf = {\n",
    "            \"accuracy\": float(accuracy_score(y_inf_bin, y_hat_inf)),\n",
    "            \"precision\": float(precision_recall_fscore_support(y_inf_bin, y_hat_inf, average=\"binary\", zero_division=0)[0]),\n",
    "            \"recall\": float(precision_recall_fscore_support(y_inf_bin, y_hat_inf, average=\"binary\", zero_division=0)[1]),\n",
    "            \"f1\": float(precision_recall_fscore_support(y_inf_bin, y_hat_inf, average=\"binary\", zero_division=0)[2]),\n",
    "            \"roc_auc\": float(roc_auc_score(y_inf_bin, y_prob_inf)),\n",
    "            \"pr_auc\": float(average_precision_score(y_inf_bin, y_prob_inf)),\n",
    "        }\n",
    "        print(\"=== INFERENCE metrics (threshold=0.5) ===\")\n",
    "        print(metrics_inf)\n",
    "    else:\n",
    "        print(\"Inference labels missing; computed probabilities only.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "890377a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /home/almog/projects/sensor-analysis-assignment/outputs/metrics_snapshot.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Minimal monitoring snapshot (repo-local outputs/)\n",
    "snapshot = {\n",
    "    \"dataset\": \"Nexar IMU\",\n",
    "    \"test\": {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1\": float(f1),\n",
    "        \"roc_auc\": float(roc),\n",
    "        \"pr_auc\": float(pr),\n",
    "        \"threshold\": 0.5,\n",
    "    },\n",
    "}\n",
    "\n",
    "out_path = Path(\"outputs\") / \"metrics_snapshot.json\"\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(out_path, \"w\") as f:\n",
    "    json.dump(snapshot, f, indent=2)\n",
    "\n",
    "print(\"Wrote:\", out_path.resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
